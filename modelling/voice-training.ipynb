{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, History\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3f0a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -q transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ef3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n",
    "Crema = \"/kaggle/input/cremad/AudioWAV/\"\n",
    "Tess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
    "Savee = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "for dir in ravdess_directory_list:\n",
    "    actor = os.listdir(Ravdess + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        file_emotion.append(int(part[2]))\n",
    "        file_path.append(Ravdess + dir + '/' + file)\n",
    "        \n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "Ravdess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be0a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "crema_directory_list = os.listdir(Crema)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in crema_directory_list:\n",
    "    file_path.append(Crema + file)\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Crema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2fdb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_directory_list = os.listdir(Tess)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + dir)\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "        if part=='ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "        \n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Tess_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e731ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "savee_directory_list = os.listdir(Savee)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(Savee + file)\n",
    "    part = file.split('_')[1]\n",
    "    ele = part[:-6]\n",
    "    if ele=='a':\n",
    "        file_emotion.append('angry')\n",
    "    elif ele=='d':\n",
    "        file_emotion.append('disgust')\n",
    "    elif ele=='f':\n",
    "        file_emotion.append('fear')\n",
    "    elif ele=='h':\n",
    "        file_emotion.append('happy')\n",
    "    elif ele=='n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif ele=='sa':\n",
    "        file_emotion.append('sad')\n",
    "    else:\n",
    "        file_emotion.append('surprise')\n",
    "        \n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Savee_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\n",
    "data_path.to_csv(\"data_path.csv\",index=False)\n",
    "data_path.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4375b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = data_path.Path\n",
    "emotions = data_path.Emotions\n",
    "\n",
    "def get_waveform(file_path, sample_rate):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(file_path, sr=sample_rate)\n",
    "    return y\n",
    "\n",
    "audio_data = []\n",
    "\n",
    "for file_path in paths:\n",
    "    waveform = get_waveform(file_path, sample_rate=16000)\n",
    "    audio_data.append(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ef8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Features, ClassLabel, Value, Audio\n",
    "from transformers import AutoFeatureExtractor\n",
    "import evaluate\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "#     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "paths = data_path.Path\n",
    "emotions = data_path.Emotions\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoded_emotion = encoder.fit_transform(np.array(emotions).reshape(-1,1))\n",
    "\n",
    "data = {\"audio\": audio_data, \"label\": encoded_emotion.astype(int)}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True, padding=True\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "\n",
    "split = dataset.train_test_split(test_size=0.2)\n",
    "encoded_split = split.map(preprocess_function, remove_columns=\"audio\", batched=True)\n",
    "encoded_split = encoded_split.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686eaf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, balanced_accuracy_score\n",
    "from datasets import load_metric\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    acc = accuracy_score(p.label_ids, preds)\n",
    "    bal_acc = balanced_accuracy_score(p.label_ids, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.unique(data_path.Emotions)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(encoder.classes_):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = len(id2label)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export WANDB_DISABLED=true\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca4f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_mind_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    do_eval=True,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_split[\"train\"],\n",
    "    eval_dataset=encoded_split[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/kaggle/working/model_saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652531e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(encoded_split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e5913",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(encoded_split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206541c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_ids = pred.predictions.argmax(-1)\n",
    "id2label = model.config.id2label\n",
    "predicted_labels = [id2label[str(class_id)] for class_id in predicted_class_ids]\n",
    "# print(f\"Predicted labels: {predicted_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f00f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "true_class_ids = encoded_split['test'][\"labels\"]\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(true_class_ids, predicted_class_ids)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label2id)\n",
    "print(f\"Test Accuracy: {accuracy_score(true_class_ids, predicted_class_ids)*100:.2f}%\")\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d6116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
